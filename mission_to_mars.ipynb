{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [C:\\Users\\m_che\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.15\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Title of the latest article\n",
    "news_title = browser.find_by_css('div.content_title')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the content of the latest article\n",
    "news_p = browser.find_by_css('div.article_teaser_body')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://spaceimages-mars.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find full size .jpg image\n",
    "browser.find_by_text(' FULL IMAGE').click()\n",
    "\n",
    "# Save the image to a variable\n",
    "featured_image_url = browser.find_by_css('img.fancybox-image')[0]['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://galaxyfacts-mars.com/'\n",
    "\n",
    "# Extracting second table \n",
    "table = pd.read_html(url)[0] \n",
    "\n",
    "# Naming the columns\n",
    "table.columns=['Description','Mars','Earth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to an HTML table string\n",
    "html_format_table = table.to_html(index=False, classes=['table','table-striped','table-responsive'])\n",
    "\n",
    "# Clean it up by getting rid of '\\n'\n",
    "html_format_table = html_format_table.replace('\\n', '')\n",
    "\n",
    "# Replace the align (from default 'right') to 'left'\n",
    "html_format_table = html_format_table.replace('right', 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Splinter method of getting  the images and titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://marshemispheres.com/'\n",
    "    \n",
    "# Go to main page\n",
    "browser.visit(url)\n",
    "\n",
    "# Find how many links to pictures are on the page\n",
    "links = browser.find_by_css('h3')\n",
    "links_number = len(links)-1\n",
    "\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "for x in range(links_number):\n",
    "    \n",
    "    # Get the title\n",
    "    links = browser.find_by_css('h3')\n",
    "    title = links[x].text\n",
    "    \n",
    "    # Click the link\n",
    "    links[x].click()\n",
    "    \n",
    "    # Extract picture url\n",
    "    url_link = browser.find_by_text('Sample')['href']\n",
    "    \n",
    "    # Create dictionary\n",
    "    hemisphere_image_urls.append({\"title\":title,\"img_url\":url_link})\n",
    "    \n",
    "    #Go back orginal page\n",
    "    browser.find_by_text('Back').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup version of getting the images and titles:\n",
    "(I used Splinter method in 'scrape_mars.py', but below is the BeautifulSoup method I could use to get the same result for getting Mars Hemispheres images and titles.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://marshemispheres.com/'\n",
    "\n",
    "# Retrieve page with the requests module\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = bs(response.text,'html.parser')\n",
    "\n",
    "# Examine the results, then determine element that contains sought info (titles, img urls)\n",
    "results = soup.find_all('div', class_='item')\n",
    "\n",
    "href_list = []\n",
    "title_list = []\n",
    "\n",
    "# Loop through returned results and append to the lists above\n",
    "for result in results:\n",
    "    title_list.append(result.h3.text)\n",
    "    href_list.append(url + result.a['href'])\n",
    "    \n",
    "image_url_list = []\n",
    "\n",
    "# Loop through each img to get high resolution pics urls and append to the 'image_url_list'\n",
    "for url_p in href_list:\n",
    "    response = requests.get(url_p)\n",
    "    soup = bs(response.text,'html.parser')\n",
    "    results = soup.find_all('div', class_='downloads')\n",
    "    image_url_list.append(url + results[0].find_all('a')[0]['href'])\n",
    "    \n",
    "\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Create the list of dictionaries with the image url string and the hemisphere title, and \n",
    "# append to 'hemisphere_image_urls' list\n",
    "for index in range(len(title_list)):\n",
    "    hemisphere_image_urls.append({'title':title_list[index], 'img_url':image_url_list[index]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
